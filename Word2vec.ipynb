{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, math\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = open(\"./data/text8\", \"r\").read().split(\" \")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = open(\"./data/vocab.txt\", \"r\").read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq = FreqDist(corpus).most_common(10000)\n",
    "for w in freq:\n",
    "    if not w[0] in vocabulary:\n",
    "        vocabulary.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "dimension = 100\n",
    "learning_rate = 0.1\n",
    "vocabulary_size = len(vocabulary) + 1\n",
    "negtive_sample_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary ={}\n",
    "word_list = vocabulary + [\"<unk>\"]\n",
    "count = 0\n",
    "for w in word_list:\n",
    "    dictionary[w] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [-1 for i in xrange(len(corpus))]\n",
    "word_count = [0 for i in xrange(vocabulary_size)]\n",
    "i = 0\n",
    "for w in corpus:\n",
    "    if w in dictionary:\n",
    "        words[i] = dictionary[w]\n",
    "    else:\n",
    "        words[i] = vocabulary_size - 1\n",
    "    word_count[words[i]] += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_rate = 0.75\n",
    "sum_cover = []\n",
    "for i, count in enumerate(word_count[:-1]):\n",
    "    x = count**norm_rate\n",
    "    if (i == 0): sum_cover = [x]\n",
    "    else: sum_cover.append(sum_cover[-1]+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if (x > 10): return 1.0\n",
    "    if (x <-10): return 0.0\n",
    "    return 1.0 / (1.0 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_word_by_cover(c):\n",
    "    l = 1\n",
    "    r = vocabulary_size-2\n",
    "    ans = r-2\n",
    "    while (l <= r):\n",
    "        mid = (l + r) / 2\n",
    "        if (sum_cover[mid] >= c):\n",
    "            ans = mid\n",
    "            r = mid - 1\n",
    "        else:\n",
    "            l = mid + 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_negative(pos_samples):\n",
    "    global words, sum_cover\n",
    "    neg_samples = []\n",
    "    temp = negtive_sample_size\n",
    "    for j in xrange(temp):\n",
    "        x = find_word_by_cover(np.random.rand()*sum_cover[-1])\n",
    "        if not (x in pos_samples): neg_samples.append((x, 0))\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(low=-0.5/dimension, high=0.5/dimension, size=(vocabulary_size, dimension))\n",
    "W2 = np.zeros((vocabulary_size, dimension))\n",
    "delta = np.zeros(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "progress = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 7 7489000/17005207 Loss = 0.78363563176473323"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e2438deaa84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in xrange(10):\n",
    "    word_shown = [0 for i in xrange(vocabulary_size)]\n",
    "    learning_rate = 0.1/np.sqrt(epoch+1)\n",
    "    for i in xrange(len(words)):\n",
    "        k = words[i]\n",
    "        progress = i+1\n",
    "        word_shown[k] += 1\n",
    "        if (k == vocabulary_size -1): continue\n",
    "        loss = 0\n",
    "        pos = words[i-random.randint(1, window_size)+1 : i] + words[i+1 : i+random.randint(1, window_size)]\n",
    "\n",
    "        for w in pos:\n",
    "            if w == vocabulary_size -1: continue\n",
    "            delta.fill(0)\n",
    "            samples = [(w, 1)] + get_negative(pos)\n",
    "            for token, label in samples:\n",
    "                y = np.dot(W1[k], W2[token])\n",
    "                p = sigmoid(y)\n",
    "                loss += (-2*label + 1) * np.log(p+1)\n",
    "                g = -learning_rate * (p - label)\n",
    "                delta += g * W2[token]\n",
    "                W2[token] += g * W1[k]\n",
    "            W1[k] += delta\n",
    "\n",
    "        if (progress % 1000 == 0):\n",
    "            sys.stdout.flush()\n",
    "            opt = \"\\r\\rtraining epoch \"+str(epoch)+\" \"+str(progress)+\"/\"+str(len(words))+\" \"\n",
    "            opt += \"Loss = \"+str(loss)\n",
    "            sys.stdout.write(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_embedding(word_list, W1):\n",
    "file = open(\"vectors.txt\", \"w\")\n",
    "for i in xrange(4894):\n",
    "    print >> file, word_list[i], \" \".join([str(w) for w in W1[i]])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
